{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Unitree G1 Robot Training with Mujoco (Colab-Compatible)\n",
    "\n",
    "This notebook trains a locomotion policy for the Unitree G1 humanoid robot using:\n",
    "- **Mujoco** physics simulation (CPU/GPU compatible, works on Colab)\n",
    "- **PPO** reinforcement learning algorithm (rsl_rl)\n",
    "- **No Isaac Gym required** - runs entirely on Google Colab free tier\n",
    "\n",
    "After training, you can download the policy and visualize it locally with Mujoco.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Runtime**: Go to `Runtime > Change runtime type` and select `GPU` (T4 recommended)\n",
    "2. **Run all cells** in order\n",
    "3. **Training time**: ~13 hours for 10,000 iterations (can stop earlier and resume later)\n",
    "4. **Download trained models** at the end\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-deps"
   },
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install required packages for Mujoco-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q mujoco==3.2.3\n",
    "!pip install -q scipy\n",
    "!pip install -q pyyaml\n",
    "!pip install -q tensorboard\n",
    "!pip install -q rsl-rl-lib\n",
    "!pip install -q matplotlib\n",
    "!pip install -q numpy\n",
    "\n",
    "print(\"\\nâœ… Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone-repo"
   },
   "source": [
    "## 2. Clone Repository and Install Package\n",
    "\n",
    "Clone the modified unitree_rl_mugym repository with Mujoco support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove old clone if exists\n",
    "if os.path.exists('unitree_rl_mugym'):\n",
    "    !rm -rf unitree_rl_mugym\n",
    "    print(\"Removed old repository\")\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/julienokumu/unitree_rl_mugym.git\n",
    "print(\"\\nâœ… Repository cloned!\")\n",
    "\n",
    "# Change to repo directory\n",
    "%cd unitree_rl_mugym\n",
    "\n",
    "# Install package (without Isaac Gym dependencies)\n",
    "!pip install -q -e . --no-deps\n",
    "\n",
    "print(\"\\nâœ… Package installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify"
   },
   "source": [
    "## 3. Verify Installation\n",
    "\n",
    "Check that Mujoco and the package are properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-code"
   },
   "outputs": [],
   "source": [
    "import mujoco\n",
    "import torch\n",
    "from legged_gym.envs.g1.mujoco_g1_env import MujocoG1Robot\n",
    "from legged_gym.envs.g1.mujoco_g1_config import MujocoG1RoughCfg, MujocoG1RoughCfgPPO\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"Mujoco version: {mujoco.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train"
   },
   "source": "## 4. Start Training\n\nTrain the G1 locomotion policy using PPO with Mujoco simulation.\n\n**Training Parameters:**\n- **Environments**: 512 parallel simulations\n- **Device**: CUDA (GPU)\n- **Max iterations**: 1000 (adjustable - see options below)\n- **Save interval**: Every 50 iterations\n- **Expected time**: ~2 hours for 50 iterations, ~3.5 hours for 100 iterations\n\n**Training Options:**\n- **Quick test** (5 iterations): ~10 minutes - verify everything works\n- **Short session** (50 iterations): ~2 hours - fits Colab free tier nicely\n- **Medium session** (100 iterations): ~3.5 hours - good progress\n- **Full training** (1000 iterations): ~35 hours - requires multiple resume sessions\n\n**Notes:**\n- Colab free tier provides 12-hour sessions - plan accordingly\n- Models are automatically saved every 50 iterations\n- You can interrupt training anytime (Ctrl+C or stop button)\n- Resume training later from any checkpoint\n- Monitor progress in real-time with TensorBoard (next cell)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-code"
   },
   "outputs": [],
   "source": "# Start training with Mujoco\n# Default: 1000 iterations (~35 hours total, save every 50)\n# For 2-hour session: Use --max_iterations 50\n# For quick test: Use --max_iterations 5\n!python legged_gym/scripts/train_mujoco.py \\\n    --device cuda \\\n    --num_envs 512 \\\n    --max_iterations 50\n\nprint(\"\\nâœ… Training completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor"
   },
   "source": [
    "## 5. Monitor Training Progress (Optional)\n",
    "\n",
    "Launch TensorBoard to visualize training metrics in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tensorboard"
   },
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch TensorBoard\n",
    "%tensorboard --logdir logs/g1_colab_training\n",
    "\n",
    "print(\"\\nðŸ“Š TensorBoard is running above. You can monitor:\")\n",
    "print(\"  - Mean reward per episode\")\n",
    "print(\"  - Value function loss\")\n",
    "print(\"  - Policy loss and entropy\")\n",
    "print(\"  - Individual reward components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "resume"
   },
   "source": "## 6. Resume Training (Optional)\n\nIf training was interrupted, you can resume from the latest checkpoint."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "resume-code"
   },
   "outputs": [],
   "source": [
    "# Resume training from last checkpoint\n",
    "!python legged_gym/scripts/train_mujoco.py \\\n",
    "    --device cuda \\\n",
    "    --num_envs 512 \\\n",
    "    --resume \\\n",
    "    --load_run -1 \\\n",
    "    --checkpoint -1\n",
    "\n",
    "print(\"\\nâœ… Resumed training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": "## 7. Convert Checkpoint to TorchScript (Required for Local Visualization)\n\nThe training saves checkpoints as PyTorch state dictionaries, but the deployment script needs TorchScript format. This cell automatically converts your latest checkpoint."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-code"
   },
   "outputs": [],
   "source": "import torch\nimport copy\nimport os\nimport glob\nfrom legged_gym.envs.g1.mujoco_g1_config import MujocoG1RoughCfg, MujocoG1RoughCfgPPO\nfrom rsl_rl.modules import ActorCriticRecurrent\n\nclass PolicyExporterLSTM(torch.nn.Module):\n    \"\"\"Export LSTM policy to TorchScript\"\"\"\n    def __init__(self, actor_critic):\n        super().__init__()\n        self.actor = copy.deepcopy(actor_critic.actor)\n        self.is_recurrent = actor_critic.is_recurrent\n        self.memory = copy.deepcopy(actor_critic.memory_a.rnn)\n        self.memory.cpu()\n        self.register_buffer(f'hidden_state', torch.zeros(self.memory.num_layers, 1, self.memory.hidden_size))\n        self.register_buffer(f'cell_state', torch.zeros(self.memory.num_layers, 1, self.memory.hidden_size))\n\n    def forward(self, x):\n        out, (h, c) = self.memory(x.unsqueeze(0), (self.hidden_state, self.cell_state))\n        self.hidden_state[:] = h\n        self.cell_state[:] = c\n        return self.actor(out.squeeze(0))\n\n    @torch.jit.export\n    def reset_memory(self):\n        self.hidden_state[:] = 0.\n        self.cell_state[:] = 0.\n\n# Find latest checkpoint\nlog_dir = \"logs/g1_colab_training\"\ncheckpoints = sorted(glob.glob(os.path.join(log_dir, \"*/model_*.pt\")))\n\nif checkpoints:\n    latest_checkpoint = checkpoints[-1]\n    checkpoint_dir = os.path.dirname(latest_checkpoint)\n    export_dir = os.path.join(checkpoint_dir, \"exported\")\n    \n    print(f\"Converting checkpoint: {latest_checkpoint}\")\n    print(f\"Output directory: {export_dir}\")\n    \n    # Load checkpoint\n    checkpoint = torch.load(latest_checkpoint, map_location='cpu')\n    print(f\"âœ“ Loaded checkpoint from iteration {checkpoint.get('iter', 'unknown')}\")\n    \n    # Get config\n    env_cfg = MujocoG1RoughCfg()\n    train_cfg = MujocoG1RoughCfgPPO()\n    \n    # Create policy network\n    actor_critic = ActorCriticRecurrent(\n        num_actor_obs=env_cfg.env.num_observations,\n        num_critic_obs=env_cfg.env.num_privileged_obs or env_cfg.env.num_observations,\n        num_actions=env_cfg.env.num_actions,\n        actor_hidden_dims=train_cfg.policy.actor_hidden_dims,\n        critic_hidden_dims=train_cfg.policy.critic_hidden_dims,\n        activation=train_cfg.policy.activation,\n        rnn_type=train_cfg.policy.rnn_type,\n        rnn_hidden_size=train_cfg.policy.rnn_hidden_size,\n        rnn_num_layers=train_cfg.policy.rnn_num_layers,\n    )\n    \n    # Load weights\n    actor_critic.load_state_dict(checkpoint['model_state_dict'])\n    actor_critic.eval()\n    print(\"âœ“ Weights loaded successfully\")\n    \n    # Export to TorchScript\n    os.makedirs(export_dir, exist_ok=True)\n    exporter = PolicyExporterLSTM(actor_critic)\n    exporter.to('cpu')\n    traced_script_module = torch.jit.script(exporter)\n    \n    policy_path = os.path.join(export_dir, 'policy_1.pt')\n    traced_script_module.save(policy_path)\n    \n    print(f\"âœ“ Exported TorchScript policy to: {policy_path}\")\n    print(f\"\\nâœ… Conversion complete! Download this file for local visualization.\")\nelse:\n    print(\"âŒ No checkpoints found. Make sure training has completed at least one save interval.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compress"
   },
   "source": "## 8. Download Trained Policy for Local Visualization\n\nDownload the converted TorchScript policy to test locally with Mujoco visualization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compress-code"
   },
   "outputs": [],
   "source": "from google.colab import files\nimport os\nimport glob\n\n# Find the exported TorchScript policy\nlog_dir = \"logs/g1_colab_training\"\nexported_policies = sorted(glob.glob(os.path.join(log_dir, \"*/exported/policy_1.pt\")))\n\nif exported_policies:\n    latest_policy = exported_policies[-1]\n    run_name = latest_policy.split('/')[2]  # Extract run directory name\n    \n    print(f\"Found exported policy: {latest_policy}\")\n    print(f\"Run: {run_name}\")\n    print(f\"Size: {os.path.getsize(latest_policy) / 1024:.2f} KB\\n\")\n    \n    print(\"Downloading policy...\")\n    files.download(latest_policy)\n    \n    print(\"\\nâœ… Download complete!\")\n    print(\"\\n\" + \"=\"*60)\n    print(\"LOCAL VISUALIZATION INSTRUCTIONS\")\n    print(\"=\"*60)\n    print(\"\\n1. Install minimal dependencies on your local machine:\")\n    print(\"   pip install mujoco==3.2.3 torch pyyaml numpy\")\n    print(\"\\n2. Clone the repository:\")\n    print(\"   git clone https://github.com/julienokumu/unitree_rl_mugym.git\")\n    print(\"   cd unitree_rl_mugym\")\n    print(\"\\n3. Run visualization with downloaded policy:\")\n    print(\"   python deploy/deploy_mujoco/deploy_mujoco.py g1.yaml \\\\\")\n    print(f\"       --policy ~/Downloads/policy_1.pt\")\n    print(\"\\n\" + \"=\"*60)\nelse:\n    print(\"âŒ No exported policy found!\")\n    print(\"\\nMake sure you ran the conversion cell above.\")\n    print(\"If training just completed, run the conversion cell first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "info"
   },
   "source": "---\n\n## Training Information\n\n### What the Robot Learns\n\nThe G1 humanoid robot learns to:\n- **Walk** in different directions (forward, backward, sideways)\n- **Turn** at various angular velocities\n- **Maintain balance** and upright posture\n- **Coordinate leg movements** for stable bipedal locomotion\n- **Follow velocity commands** (vx, vy, vyaw)\n\n### Reward Components\n\nThe policy is trained to maximize rewards for:\n- **Velocity tracking**: Following commanded velocities\n- **Survival**: Staying upright and alive\n- **Contact patterns**: Proper gait with alternating foot contact\n- **Smooth motion**: Minimal joint acceleration and action changes\n\nAnd minimize penalties for:\n- **Energy consumption**: High torques\n- **Undesired contacts**: Body parts touching the ground\n- **Joint limits**: Approaching position/velocity limits\n- **Orientation errors**: Tilting or falling\n\n### Network Architecture\n\n**Actor (Policy Network):**\n- Input: 47-dim observations (velocities, orientation, commands, joint states, phase)\n- LSTM: 47 â†’ 64 (temporal processing)\n- MLP: 64 â†’ 32 â†’ 12 actions (joint position targets)\n\n**Critic (Value Network):**\n- Input: 50-dim privileged observations (includes ground truth velocities)\n- LSTM: 50 â†’ 64 (temporal processing)\n- MLP: 64 â†’ 32 â†’ 1 value (state value estimate)\n\n### Hyperparameters\n\n- **Algorithm**: PPO (Proximal Policy Optimization)\n- **Environments**: 512 parallel simulations\n- **Steps per env**: 24 steps per rollout\n- **Max iterations**: 1000 (adjustable, ~35 hours total)\n- **Save interval**: 50 iterations (~2 hours)\n- **Learning rate**: 0.001\n- **Discount (Î³)**: 0.99\n- **GAE Î»**: 0.95\n- **Clip parameter**: 0.2\n- **Entropy coefficient**: 0.01\n\n### Training Time Estimates (T4 GPU)\n\n- **5 iterations**: ~10 minutes (quick test)\n- **50 iterations**: ~2 hours (perfect for Colab free tier)\n- **100 iterations**: ~3.5 hours (good progress)\n- **500 iterations**: ~17.5 hours (needs resume)\n- **1000 iterations**: ~35 hours (multiple sessions required)\n\n---\n\n## Troubleshooting\n\n**Training is slow:**\n- Make sure you selected GPU runtime (Runtime > Change runtime type > GPU)\n- T4 GPU gives ~98 steps/s, which is expected\n\n**Out of memory:**\n- Reduce `--num_envs` to 256 or 128\n- Restart runtime and try again\n\n**Session disconnected:**\n- Colab free tier has time limits (~12 hours)\n- Download checkpoints periodically\n- Resume training from last checkpoint when reconnected\n\n**Import errors:**\n- Re-run the dependency installation cell\n- Restart runtime if needed\n\n---\n\n## Repository\n\n**GitHub**: [julienokumu/unitree_rl_mugym](https://github.com/julienokumu/unitree_rl_mugym)\n\nBased on the original [unitreerobotics/unitree_rl_gym](https://github.com/unitreerobotics/unitree_rl_gym) with modifications for Mujoco compatibility and Google Colab support.\n\n---\n\n**Happy Training! ðŸ¤–ðŸš€**"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}